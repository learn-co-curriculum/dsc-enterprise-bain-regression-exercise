{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - data is coming from this kaggle depot:\n",
    "https://www.kaggle.com/harlfoxem/housesalesprediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "// Ensure our output is sufficiently large\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 1\n",
    "\n",
    "* Load libraries\n",
    "* Load data\n",
    "* Explore data for quality\n",
    "* Data clean-up\n",
    "* Explore data for Linear Regression Suitability\n",
    "* Mechanical Feature Engineering\n",
    "* Run Linear Regression\n",
    "* Explore coefficients\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emnsure all charts plotted automatically\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "import sklearn\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Don't truncate dataframe displays columwise\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Engineer Data: Load the data\n",
    "\n",
    "You load the CSV data into a Pandas object. This is a common Python library to work with data in row/column format, like .csv files.\n",
    "\n",
    "Print out top 10 rows just to get a feel for the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__You should see:__\n",
    "\n",
    "* 21613 rows of data across 21 columns (different variables)\n",
    "* All data appears to be numerical\n",
    "\n",
    "__Helpful  decoder ring for what each variable means (stuff you would get from the client)__\n",
    "\n",
    "* __id__ a notation for a house\n",
    "* __date__ Date house was sold\n",
    "* __price__ Price is prediction target\n",
    "* __bedrooms__ Number of Bedrooms/House\n",
    "* __bathrooms__ Number of bathrooms/House\n",
    "* __sqft_living__ square footage of the home\n",
    "* __sqft_lot__ square footage of the lot\n",
    "* __floors__ Total floors (levels) in house\n",
    "* __waterfront__ House which has a view to a waterfront\n",
    "* __view__ quality of view - higher == better\n",
    "* __condition__ How good the condition is ( Overall ) - higher == better\n",
    "* __grade__ overall grade given to the housing unit, based on King County grading system - higher == better\n",
    "* __sqft_above__ square footage of house apart from basement\n",
    "* __sqft_basement__ square footage of the basement\n",
    "* __yr_built__ Built Year\n",
    "* __yr_renovated__ Year when house was renovated\n",
    "* __zipcode__ zip\n",
    "* __lat__ Latitude coordinate\n",
    "* __long__ Longitude coordinate\n",
    "* __sqft_living15__ Living room area in 2015(implies-- some renovations) This might or might not have affected the lotsize area\n",
    "* __sqft_lot15__ lotSize area in 2015(implies-- some renovations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the data for quality: \n",
    "\n",
    "* Check  for nulls\n",
    "* Check for outliers on data that is numerical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for nulls\n",
    "\n",
    "* Hint - there is insnull() function as part of dataframe, that you can use to check each field\n",
    "* Thne you can se sum() across columns (axis = 0) to see how often it happens in each column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__:\n",
    "\n",
    "* All the columns have count of nulls at 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for anomalies on numerical data\n",
    "* create a list of numerical columns (i.e.int64 and float 64)\n",
    "    * df.select_dtypes(include=['int64','float64']).columns\n",
    "    \n",
    "* run a loop on that list and do a histogram for each (or other visualiation you think would be apt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__One obvious issue:__\n",
    "    * Year renovated - appears to use two different standards \n",
    "\n",
    "__A few suspicious items:__\n",
    "    * Price - long tail, which can drive huge absolute errors\n",
    "    * bedrooms - long-tail and apparently some have 0?\n",
    "    * bathrooms - some have 0?\n",
    "    * sqft_living - long-tail\n",
    "    * sqft_lot - long-tail\n",
    "\n",
    "__Date is only non-numberical - we should check distribution__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data checks\n",
    "\n",
    "* renovation year\n",
    "* price\n",
    "* bedrooms\n",
    "* bathroom\n",
    "* sqft_living\n",
    "* sqft_lot\n",
    "\n",
    "Things to do:\n",
    "* Draw histograms but force more granularity (e.g. num of bins)\n",
    "* do value_counts to see what distribution looks like for outliers\n",
    "* etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__:\n",
    "* Renovation year - looks like it's all OK - 0 means no renovation\n",
    "* Price - looks like it may be just a few outliers over $3M - need to check how many\n",
    "* Bedrooms - just 13 with 0 (likely studios) and 1 with 33 - likely outlier\n",
    "* Bathrooms - 10 with 0 - seems not possible\n",
    "* sqft_living and sqft_lot - may be just very few outliers - need to check how many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking data - deeper click\n",
    "\n",
    "* long-tail of prices\n",
    "* bedrooms - over 30?, 0?\n",
    "* bathrooms - 0?\n",
    "* sqftliving - over 8000?\n",
    "* sqft_lot - over 250K, 500K\n",
    "\n",
    "* Do counts of issues\n",
    "* Display problematic ranges (i.e. literally use display)\n",
    "* Etc - really anything you think will give you a better sens of rht edata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "    \n",
    "* Price - looks OK - 45 houses over 3M USD and 8 over 5M USD\n",
    "* Bedrooms:\n",
    "    * House with 33 bedrooms is clearly a typo since modest sized home with average price - probably meant as 3\n",
    "    * Houses with 0 bedrooms - some are suspicious, especially those with 0 bathrooms or those that are huge and multiple bathrooms\n",
    "* Bathrooms - think 0 bathroom houses make no sense - these should be dropped\n",
    "* Sqft_living - all makes sense - all outliers have lots of rooms and high price\n",
    "* Sqft_lot - seems too many with big lots (100+) and those with hugest lots (over 1M) are not ver expensive and very little correlation between lot size and price, even when controlling for size of house. This will likely cause large errors, but let's leave it be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking date distributrion\n",
    "* Convert all dates to datetime64 type\n",
    "* Group by year (df_date.dt.year) and month (df_date.dt.year) and count\n",
    "* Plot as bar\n",
    "\n",
    "* Or Google until you find the asnwer :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* Looks good, data is ineeded from interval stated and distribution is reasoanble (peak in late spring/early summer in the Northern hemisphere)\n",
    "* Suspect we're missign some days in May 2015\n",
    "* It appears there is some seasonality to prices (i.e. lower in winter, higher in summer) - we should add this as a feature in later sprints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data clean-up\n",
    "\n",
    "__To Do__:\n",
    "* Modify listing with 33 bedrooms to be 3\n",
    "* Remove listings with 0 bathrooms (simply select sub-section of df where bathrooms > 0)\n",
    "\n",
    "__WARNING: YOU SHOULD NEVER DO THIS IN PRACTICE. YOU WANT TO FIRST CHECK WITH CLIENT__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check data for Linear Regression Suitability\n",
    "* Check for colinearity\n",
    "* Check for linear realtionship"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for co-linearity\n",
    "\n",
    "* Create a list of numerical features (Can borrow from our initial data quality check)\n",
    "* Drop id and price from list\n",
    "* Use sns.heatmap() and .corr() function of our data-frame with just numberical features to draw correlation matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__:\n",
    "\n",
    "* Variables that are highly positively correlated:\n",
    "    * sqft_above and sqft_living - we should keep sqft_living only\n",
    "    * sqft_living15 and sqft_lot15 with non-15 values - let's keep non-15 values\n",
    "\n",
    "* Variables that have high correlation, but we should probably keep:\n",
    "    * Bathrooms with sqft_living\n",
    "    * Grade with sqft_living\n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for linear relationships\n",
    "\n",
    "* We should only show relationship for variables we know will have linear relationship and where no correlation:\n",
    "\n",
    "* Createa a list of numerical variables (Can borrown from previous step)\n",
    "* remove varialbes that we know can't work or have correlation:\n",
    "    * Won't work: Lat, long\n",
    "    * Might work, but requires one-hot encoding: zip-code\n",
    "    * Corelation: sqft_living15, sqft_lot15\n",
    "    \n",
    "* Run a loop and do a sns.jointplot() on price + each variable\n",
    "    * extra - you could do 1 LR OLS regression plot to see if LR will pick-up on it sns.regplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see and what to do about it__:\n",
    "* Bedrooms - somewhat liner to 6, but then it's not -> make it one-hot\n",
    "* Bathrooms - mostly linear -> keep as is\n",
    "* sqft_living - mostly linear -> keep as is, but add a transofrm to bring large values into line (e.g.)\n",
    "* sqft_lot - not at all linear for small lots, somewhat linear for big ones -> make it one-hot into several increments\n",
    "* floors - no relationshop - > drop\n",
    "* Waterfront - small relationship -> keep\n",
    "* View - weak relationship ->drop\n",
    "* Condiiton - no relationship -> drop\n",
    "* Grade - solid link but not super-linear -> make into one-hot as will be hard to make a transformaiton\n",
    "* sqft_basement -> the no basement is tripping us up -> requires thoughful FE, let's adress in future sprint\n",
    "* yr_built - no relationship -> drop\n",
    "* yr_renovated - would need to look at what it looks like within only those renovated and then decide\n",
    "\n",
    "__We need to explore just a bit more to figure out what to do with following:__\n",
    "\n",
    "* sqft-living - how to best transform larger values?\n",
    "    * Let's try squaring the sqft_living and then normalizing\n",
    "* sqft_lot - where to draw buckets?\n",
    "    * let's draw 3 seperate scatters in 50K, 50K to 500K and over 500K\n",
    "* yr_renovated - is there relationship for those renovated\n",
    "    * Let's show what non-zero renovation looks like\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see and do:__\n",
    "\n",
    "* It looks likes squaring the sqft_living makes it far more linear - we should keep\n",
    "* There is not realtionship in sqft_lot -> we should drop for now\n",
    "* Yr_renovated -> appears to be a relationship, may make sense to add as a feature in future sprints as one hot (for no renovaiton, and then buckets for decades since renovated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mechanical feature engineering\n",
    "* We should start with a few features and then add them slowly - use conclusions from above as starting point\n",
    "* Will need to make some into one-hot (e.g. bedrooms, grade)\n",
    "* For others will need custom transfrom (E.g. squaring sqft_living)\n",
    "* For some continous variables, probably a good idea to normalize (e.g. number of bathrooms)\n",
    "* Some require nothing (e.g. WAterfront - already one-hot :))\n",
    "* Some are interestign but we will puruse in future sprints (e.g. sqft_basement, yr_renovated, month)\n",
    "\n",
    "* Since few variables - you can just list each individually and what transformation(s) you want to do\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Linear Model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data\n",
    "\n",
    "* Let's split at 80/20 train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function train_test_split, splits the arguments into two sets.\n",
    "X_train, X_test, y_train, y_test = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model and see regression results on train test-sets\n",
    "\n",
    "Use statsmodels OLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_lm_model = None\n",
    "\n",
    "# Now, train the model\n",
    "sm_lm_model_fit_s1 = sm_lm_model.fit()\n",
    "\n",
    "\n",
    "display(sm_lm_model_fit_s1.summary())\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see__:\n",
    "\n",
    "* Adjusted R2 of .633 is not bad for first attempt\n",
    "\n",
    "__Interpreting coefficients__:\n",
    "\n",
    "* const is 614K which is effectively average price, so coefficients will be things to move it higher or lower\n",
    "\n",
    "* Coefficients are a little difficult to interpret for some because we transfomed them (E.g. sqft_living is squared + normalized and bathrooms are normalized)\n",
    "\n",
    "* Most make sense, but a few suspicious items:\n",
    "    * More bathrooms drives house price down - opposite of what we saw -> this is likely due to co-linearity of sqft_living (0.76 correlation)\n",
    "    * Grades don't follow a linear pattern at lower end of grade scale (i.e. lowest grade doesn't take away as much value as higher ones oneS) -> likely overfitting\n",
    "    \n",
    "* Statistically speaking some should be killed b/c P>[t] is not less then .05 (95% CI)\n",
    "    * Bedrooms 0 - massive range of where coefficient could be\n",
    "    * Bedrooms 4,7,8,9,10 and 11 - likely driven by low N\n",
    "    * Grade 3 and 9 - likely driven by low N\n",
    "    \n",
    "* Interesting tid-bits\n",
    "    * Being waterfront adds 800K dollars to the house value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine accuracy on test data-set\n",
    "* Run prediciton on un-seen data and see average error on seen and unseen data\n",
    "    * model_fit.predict()\n",
    "        * Remember to use sm.addconstant(X_train) so that matrices match\n",
    "        \n",
    "* For MAE, you can use sklearn.metrics.mean_absolute_error(y_train, y_predicted)\n",
    "    * just make sure both matrices are in same shape\n",
    "* For MAPE, you need to write your own function (Remember formula is Average of ABS(actual-predicted)/true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how well we did\n",
    "def mean_absolute_percentage_error(y_true, y_pred): \n",
    "    pass\n",
    "\n",
    "y_lr_train = None\n",
    "y_lr_test = None\n",
    "\n",
    "# display(Markdown('###### Performance on training set'))\n",
    "display('Linear model train set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_train.values.ravel(), y_lr_train.values)))\n",
    "display('Linear model train set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_train.values.ravel(), y_lr_train.values)*100))\n",
    "\n",
    "# display(Markdown('###### Performance on test set'))\n",
    "display('Linear model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test.values.ravel(), y_lr_test.values)))\n",
    "display('Linear model train set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_test.values.ravel(), y_lr_test.values)*100))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* MAE of ~150,000 dollars\n",
    "* MAPE of 32% is decent first attempt, but not great\n",
    "* test set error is close enough to train that not too worried about over-fittign just yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test that model\n",
    "\n",
    "* Check residuals are normally distributed across all estiamtes\n",
    "* Check residuals are normally distributed by each point\n",
    "* Check no heteroskedacity \n",
    "\n",
    "* you'll likely need to calcualte residuals in absolute and as percentage in a seperate data-frame\n",
    "* Then you cna just use histogram and scatter plots to see what's going on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Provided, but pay attention--you'll have to do this on your own later in the exercise!\n",
    "df_residuals = y_train.copy()\n",
    "df_residuals['residual_norm'] = df_residuals.price - y_lr_train.values\n",
    "df_residuals['residual_perc'] = (df_residuals.price - y_lr_train.values)/df_residuals.price\n",
    "df_residuals['residual_perc_abs'] = np.abs(df_residuals.price - y_lr_train.values)/df_residuals.price\n",
    "\n",
    "\n",
    "display(\"Distribution of residuals\")\n",
    "plt.hist(df_residuals['residual_norm'])\n",
    "plt.show()\n",
    "plt.hist(df_residuals['residual_perc'])\n",
    "plt.show()\n",
    "\n",
    "display(\"Residuals vs price\")\n",
    "plt.scatter(df_residuals.price, df_residuals.residual_perc)\n",
    "plt.show()\n",
    "display(\"Residuals vs price for homes under 1M USD\")\n",
    "plt.scatter(df_residuals[df_residuals.price <1000000].price, df_residuals[df_residuals.price <1000000].residual_perc)\n",
    "plt.show()\n",
    "display(\"Residuals vs price for homes over 1M USD\")\n",
    "plt.scatter(df_residuals[df_residuals.price >1000000].price, df_residuals[df_residuals.price >1000000].residual_perc)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* We definitely have a few problems here\n",
    "* Residuals are not normally distribvuted - more negative errors\n",
    "* Residuals are not normally distributed across points - very negative skew at lower prices\n",
    "* There is definite heteroskedacity - bigger errors (%wise) for smaller price points - i.e. we think house should be priced up 3x more then actual sale price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore error\n",
    "\n",
    "* Let's see if these high-error low-priced homes have something in common\n",
    "* Basically add residual in percentage to the df_transformed_features_s1\n",
    "* Then predict all prices\n",
    "* Then calculate residuals\n",
    "* Then for each feature do a histogram of distribution in two steps:\n",
    "    * Do it only on homes under 1,000,000 USD \n",
    "    * Plot seperte distributions for high-error and low-error homes (let's dre the bad error at more then 75%)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example provided, but pay attention! You'll be doing this yourself later!\n",
    "\n",
    "df_error = df_transformed_features_s1.copy()\n",
    "df_error['price'] = numpy_transform_target_s1\n",
    "df_error[\"price_predicted\"] = sm_lm_model_fit_s1.predict(sm.add_constant(df_transformed_features_s1))\n",
    "df_error['residual_perc'] = (df_error.price - df_error.price_predicted)/df_error.price\n",
    "\n",
    "df_error = df_error[df_error.price < 1000000]\n",
    "\n",
    "display(df_error.head(10))\n",
    "\n",
    "features_list = list(df_error.columns)\n",
    "features_list.remove('price')\n",
    "features_list.remove('price_predicted')\n",
    "features_list.remove('residual_perc')\n",
    "\n",
    "for column in features_list:\n",
    "    plt.hist(df_error[df_error['residual_perc'] > -0.75][column], alpha=0.5, label='Normal Error',density = True)\n",
    "    plt.hist(df_error[df_error['residual_perc'] < -0.75][column], alpha=0.5, label='Big Error',density = True)\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.title(column)\n",
    "    plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* Nothing really jumps out - distribution of things with normal errors and really bad error is pretty similar\n",
    "* It's probably that one of the missing variables is playing a big role\n",
    "* We should investigate that in sprint 2 or 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint 2\n",
    "\n",
    "* Add in remaining features from sprint 1\n",
    "* Re-run OLS LR\n",
    "* Remove features that are not statistically significant\n",
    "* Re-run OLS LR\n",
    "* Join new data and explore if predicitive\n",
    "* Add new data to the data-set\n",
    "* Re-run OLS LR\n",
    "* Re-run with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add in remaining features from sprint 1\n",
    "* yr_renovated\n",
    "    * create fucntion to bucket in an intelligent way (by decade)\n",
    "    * create transform functionto apply along an entire series\n",
    "\n",
    "* sqft_basement\n",
    "    * create fucntion to bucket in an intelligent way\n",
    "    * create transform functionto apply along an entire series\n",
    "    \n",
    "* month one-hot\n",
    "    * convert date column to datetiem64\n",
    "    * create new column month that is just month porton of date\n",
    "\n",
    "* add new variables to dataframe mapper\n",
    "* run dataframe mapper\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature engineering!!!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run OLS Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train OLS LR with new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function train_test_split, splits the arguments into two sets.\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "sm_lm_model_s2_1 = None\n",
    "\n",
    "sm_lm_model_fit_s2_1 = None\n",
    "\n",
    "display(sm_lm_model_fit_s2_1.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "* Adjsut R2 got better (.65 from .63)\n",
    "* We have lots of coefficients that are not statistically significant\n",
    "\n",
    "* Let's run MAE and MAPE to see if it improves accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test re-run OLS regression\n",
    "\n",
    "* Predict train and test\n",
    "* Calculate MAPE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_lr_train = None\n",
    "y_lr_test = None\n",
    "\n",
    "# display(Markdown('###### Performance on training set'))\n",
    "display('Linear model train set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_train.values.ravel(), y_lr_train.values)))\n",
    "display('Linear model train set MAPE: {:.4f}%'.format(mean_absolute_percentage_error(y_train.values.ravel(), y_lr_train.values)))\n",
    "\n",
    "# display(Markdown('###### Performance on test set'))\n",
    "display('Linear model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test.values.ravel(), y_lr_test.values)))\n",
    "display('Linear model train set MAPE: {:.4f}%'.format(mean_absolute_percentage_error(y_test.values.ravel(), y_lr_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* Our MAPE got a bit better - from 31.6% to 30.7% - YAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove variables not statistically significant \n",
    "\n",
    "* You can get pvalues from paramter pvalues\n",
    "* Create a list of pvalues that are > 0.05 (this will be a PD Series with feature name + pvalues)\n",
    "* Get just names of those features\n",
    "* using pandas drop(feature_name, axis = 1, inplace = True) - drop those features from our df_transformed_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code provided for you, so you can see how this is done\n",
    "drop_list = sm_lm_model_fit_s2_1.pvalues[sm_lm_model_fit_s2_1.pvalues > 0.05]\n",
    "display(drop_list)\n",
    "drop_list_names = drop_list.index\n",
    "\n",
    "display(len(drop_list_names))\n",
    "\n",
    "df_transformed_features_s2_1 = df_transformed_features_s2_1.copy()\n",
    "display(df_transformed_features_s2_1.shape)\n",
    "\n",
    "for fe_to_drop in drop_list_names:\n",
    "        df_transformed_features_s2_1.drop(fe_to_drop, axis = 1, inplace = True)\n",
    "\n",
    "display(df_transformed_features_s2_1.shape)\n",
    "\n",
    "display(df_transformed_features_s2_1.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-run OLS with statistically significant variables only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function train_test_split, splits the arguments into two sets.\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "sm_lm_model_s2_1 = sm.OLS(y_train, sm.add_constant(X_train))\n",
    "\n",
    "# Fit the model\n",
    "sm_lm_model_fit_s2_1 = None\n",
    "\n",
    "# Get the summary\n",
    "display(sm_lm_model_fit_s2_1.summary())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What yuou should see:__\n",
    "\n",
    "* Adjusted R square went down a tiny bit (as expected)\n",
    "* Some new features became statistically insignificant "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MAPE, MAE of OLS Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for training and testing sets\n",
    "y_lr_train = None\n",
    "y_lr_test = None\n",
    "\n",
    "# display(Markdown('###### Performance on training set'))\n",
    "display('Linear model train set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_train.values.ravel(), y_lr_train.values)))\n",
    "display('Linear model train set MAPE: {:.4f}%'.format(mean_absolute_percentage_error(y_train.values.ravel(), y_lr_train.values)))\n",
    "\n",
    "# display(Markdown('###### Performance on test set'))\n",
    "display('Linear model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test.values.ravel(), y_lr_test.values)))\n",
    "display('Linear model train set MAPE: {:.4f}%'.format(mean_absolute_percentage_error(y_test.values.ravel(), y_lr_test.values)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* Accuracy went down a little bit, but not as bad as our first regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore error\n",
    "\n",
    "Let's see if adding new data helped with heteroskedacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* It didn't really help much..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add new data (income-level socioeconomic data)\n",
    "* Load blockgroup-data\n",
    "    * Blockgroup_avg_income_kc.csv\n",
    "* Load csv that has transaltion function between lat-long and blockgroup\n",
    "    * kc_blockgroup_lat_long.csv\n",
    "* Join data:\n",
    "    * Convert Lat-long to blockgroup\n",
    "    * Join relevant blockgroup_demographic\n",
    "* Explore data\n",
    "* Re-run with LR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load blockgroup-data & lat_long pairs translation function\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_blockgroup_data = None\n",
    "\n",
    "display(df_blockgroup_data.shape)\n",
    "display(df_blockgroup_data.head(10))\n",
    "\n",
    "df_blockgroup_pairs = None\n",
    "display(df_blockgroup_pairs.head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join lat_long to blockgroup data to enable exploration\n",
    "\n",
    "* Use merge to join blocgkroup_date with income and lat-long blockgroup dataframe\n",
    "* Test for nulls and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockgorup_joined = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore data\n",
    "* Compare lat-long income with lat-long average price of home per squarefoot\n",
    "* Fun hack - you can just use scatter since lat and long are nicely linear (use long for X, and lat for y)\n",
    "    * Then use color to see median income or average price\n",
    "    * For housing data - probably best to color not by price but dollars per square foot of living space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* there is a lot more disperity between two charts, but same general gradient\n",
    "* Red is high, blue is low and we see similar hot-spots (i.e. around downtown Seattle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-run OLS with new Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add lat_long_income into our data_set \n",
    "\n",
    "* Merge using multiple varialbes - you can simply supply left_on and right_on as a list of features to match\n",
    "* check for nulls or that you got too many when you merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__WARNING: You'll have 13 nulls, just drop for right now, but right thing to do would be to take average of 3 neareast ones...but we don't have time for that__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run new data set through our pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DataFrameMapper!\n",
    "preprocessing_steps_v4 = None\n",
    "\n",
    "\n",
    "mapper_features_v4 = None\n",
    "\n",
    "\n",
    "np_transformed_features_s2_2 = mapper_features_v4.fit_transform(df_clean_merge.copy())\n",
    "df_transformed_features_s2_2 = pd.DataFrame(data = np_transformed_features_s2_2, columns = mapper_features_v4.transformed_names_)\n",
    "\n",
    "\n",
    "mapper_target_s2_2 = DataFrameMapper([(['price'],None)])\n",
    "numpy_transform_target_s2_2 = mapper_target_s2_2.fit_transform(df_clean_merge[[TARGET]].copy())\n",
    "\n",
    "df_transformed_target_s2_2 = pd.DataFrame(data = numpy_transform_target_s2_2, columns = ['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train using new data with OLS, show coefficients and also calculate MAPE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function train_test_split, splits the arguments into two sets.\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# X_train.isnull().sum(axis = 0)\n",
    "\n",
    "# Train an OLS model from sm on the training data\n",
    "sm_lm_model_s2_2 = None\n",
    "\n",
    "# Now, fit the model (remember, statsmodel API is different than sklearn's!)\n",
    "sm_lm_model_fit_s2_2 = None\n",
    "\n",
    "\n",
    "# Now, display the summary from the model\n",
    "display(sm_lm_model_fit_s2_2.summary())\n",
    "\n",
    "# Get predictions for training and testing sets\n",
    "y_lr_train = None\n",
    "y_lr_test = None\n",
    "\n",
    "# display(Markdown('###### Performance on training set'))\n",
    "display('Linear model train set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_train.values.ravel(), y_lr_train.values)))\n",
    "display('Linear model train set MAPE: {:.4f}%'.format(mean_absolute_percentage_error(y_train.values.ravel(), y_lr_train.values)*100))\n",
    "\n",
    "# display(Markdown('###### Performance on test set'))\n",
    "display('Linear model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test.values.ravel(), y_lr_test.values)))\n",
    "display('Linear model train set MAPE: {:.4f}%'.format(mean_absolute_percentage_error(y_test.values.ravel(), y_lr_test.values)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "    \n",
    "* Adjusted R2 improved by ~5% which is also very good\n",
    "* MAPE improved by ~4% which is pretty good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if you can figure out how to explore your residuals. If you get stuck, take a look at the solution branch!\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* We continue to struggle with homoskedcity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try more sophisticated model - XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train XGRegressor using existing data pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from xgboost import XGBRegressor\n",
    "XG_model = None\n",
    "\n",
    "# Fit your model\n",
    "XG_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Generate training set predictions\n",
    "y_xg_train = None\n",
    "y_xg_train = np.reshape(y_xg_train, (-1, 1))\n",
    "\n",
    "# Generate test set predictions\n",
    "y_xg_test = None\n",
    "y_xg_test = np.reshape(y_xg_test, (-1, 1))\n",
    "\n",
    "\n",
    "display(Markdown('###### Performance on training set'))\n",
    "\n",
    "display('XGBoost model train set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_train, y_xg_train)))\n",
    "display('XGBoost model train set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_train.values, y_xg_train)*100))\n",
    "\n",
    "display(Markdown('###### Performance on test set'))\n",
    "\n",
    "display('XGBoost model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test, y_xg_test)))\n",
    "display('XGBoost model train set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_test.values, y_xg_test)*100))\n",
    "\n",
    "display(Markdown('###### Coefficients'))\n",
    "\n",
    "FEATURES = mapper_features_v4.transformed_names_\n",
    "\n",
    "pd.DataFrame({'features': FEATURES, 'importance': XG_model.feature_importances_}).sort_values('importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* MAPE of about 26% - only 0.5% better than the linear regression\n",
    "* Coefficient stength - looks like average income raises to the top, so it's possible that XGB could find some value in some of the data we decided to leave out - we should try that next\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a bit more data into our pipeline\n",
    "\n",
    "* lat, long\n",
    "* view - was weak relationship\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use DataFrameMapper!\n",
    "preprocessing_steps_s2_3 = None\n",
    "\n",
    "\n",
    "mapper_features_s2_3 = None\n",
    "\n",
    "np_transformed_features_s2_3 = mapper_features_s2_3.fit_transform(df_clean_merge.copy())\n",
    "df_transformed_features_s2_3 = pd.DataFrame(data = np_transformed_features_s2_3, columns = mapper_features_s2_3.transformed_names_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-train XGBoost, show coefficients and calculate MAPE and MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The function train_test_split, splits the arguments into two sets.\n",
    "X_train, X_test, y_train, y_test = None\n",
    "\n",
    "# Fit the model\n",
    "\n",
    "# Get predictions on your training set\n",
    "y_xg_train = None\n",
    "y_xg_train = np.reshape(y_xg_train, (-1, 1))\n",
    "\n",
    "# Get predictions on your testing set\n",
    "y_xg_test = None\n",
    "y_xg_test = np.reshape(y_xg_test, (-1, 1))\n",
    "\n",
    "\n",
    "display(Markdown('###### Performance on training set'))\n",
    "\n",
    "display('XGBoost model train set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_train, y_xg_train)))\n",
    "display('XGBoost model train set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_train.values, y_xg_train)*100))\n",
    "\n",
    "display(Markdown('###### Performance on test set'))\n",
    "\n",
    "display('XGBoost model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test, y_xg_test)))\n",
    "display('XGBoost model train set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_test.values, y_xg_test)*100))\n",
    "\n",
    "display(Markdown('###### Coefficients'))\n",
    "\n",
    "FEATURES = mapper_features_s2_3.transformed_names_\n",
    "\n",
    "pd.DataFrame({'features': FEATURES, 'importance': XG_model.feature_importances_}).sort_values('importance', ascending=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* Huge improvement - added 10%+ of accuracy and lat, and long are 2nd and 3rd most importnat feature\n",
    "* view climbed pretty high to the list of things that matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See if you can figure out how to explore your residuals. If you get stuck, take a look at the solution branch!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* Remarkably our issue with homes that are less expensive exists even with xgboost\n",
    "* It must be that we're simply missing an important predictor variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's play with depth only\n",
    "\n",
    "* Create a list of depths (e.g. from 3 to 15)\n",
    "* Run for-loop wher we re-train with different depth\n",
    "* See which one does best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example provided, you'll tune the next hyperparameter yourself in the cells below!\n",
    "\n",
    "max_depth_list = list(range(3,15))\n",
    "test_MAPE = []\n",
    "\n",
    "for max_depth in max_depth_list:\n",
    "    XG_model_tuned = XGBRegressor(max_depth = max_depth)\n",
    "    XG_model_tuned.fit(X_train, y_train)\n",
    "    \n",
    "    y_xg_test = XG_model_tuned.predict(X_test)\n",
    "    y_xg_test = np.reshape(y_xg_test, (-1, 1))\n",
    "    \n",
    "    MAPE = mean_absolute_percentage_error(y_test.values, y_xg_test)*100\n",
    "    display('Trained with max-depth of {:0f} and MAPE is {:2f}%'.format(max_depth,MAPE ))\n",
    "    test_MAPE.append(MAPE)\n",
    "    \n",
    "plt.plot(max_depth_list, test_MAPE)\n",
    "    \n",
    "\n",
    "\n",
    "#     display(Markdown('### Max_Depth = {:0f}'.format(max_depth_increment)))\n",
    "#     display(Markdown('###### Performance on test set'))\n",
    "\n",
    "#     display('XGBoost model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test, y_xg_test)))\n",
    "#     display('XGBoost model test set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_test.values, y_xg_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* loks like depth of 10 is pretty sweet - reduces error by ~3%\n",
    "* this is not surprising at all since suspect lat and long play occupy a large part of those trees to isolate neighbourhoods where similar houses get higher price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try to play with n_estimators\n",
    "\n",
    "* similar to above, go from 50 to 500 in incremenets of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimators_list = None\n",
    "test_MAPE = []\n",
    "\n",
    "\n",
    "# Now, loop through all the numbers in estimators_list. \n",
    "for None in None:\n",
    "    # For each iteration, use that number as the n_estimators parameter. Leave max_depth as 10\n",
    "    XG_model_tuned = None\n",
    "    # Fit the model\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Generate predictions\n",
    "    y_xg_test = None\n",
    "    \n",
    "    # This line reshapes the vector of predictions that the last line returned. \n",
    "    y_xg_test = np.reshape(y_xg_test, (-1, 1))\n",
    "    \n",
    "    # Calculate MAPE \n",
    "    MAPE = None\n",
    "    display('Trained with n_estimators of {:0f} and MAPE is {:2f}%'.format(n_estimators,MAPE ))\n",
    "    \n",
    "    # append the MAPE value to test_MAPE, so that we can track the MAPE performance of each iteration \n",
    "    \n",
    "    \n",
    "plt.plot(estimators_list, test_MAPE)\n",
    "    \n",
    "\n",
    "\n",
    "#     display(Markdown('### Max_Depth = {:0f}'.format(max_depth_increment)))\n",
    "#     display(Markdown('###### Performance on test set'))\n",
    "\n",
    "#     display('XGBoost model test set MAE: ${:.0f}'.format(sklearn.metrics.mean_absolute_error(y_test, y_xg_test)))\n",
    "#     display('XGBoost model test set MAPE: {:.2f}%'.format(mean_absolute_percentage_error(y_test.values, y_xg_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__What you should see:__\n",
    "\n",
    "* n_estimators of 250 seems like the sweet spot, but only reduced error by 0.07%\n",
    "* Our new error is now 13%\n",
    "* We should stop with hyper-parameter tuning now..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "481.569px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
